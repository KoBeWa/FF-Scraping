name: NFL Draft Scraping

on:
  workflow_dispatch:
    inputs:
      mode:
        description: "Was scrapen?"
        required: true
        default: "nfl_drafts"
        type: choice
        options:
          - nfl_drafts      # NFL.com History-Drafts mit BeautifulSoup
          - sleeper_drafts  # Sleeper Drafts via API
      start_year:
        description: "Startjahr (nur f체r NFL.com)"
        required: true
        default: "2015"
      end_year:
        description: "Endjahr (nur f체r NFL.com)"
        required: true
        default: "2021"
      league_id:
        description: "NFL.com League ID (z.B. 3082897)"
        required: true
        default: "3082897"
      sleeper_league_id:
        description: "Sleeper League ID (falls Sleeper gew채hlt)"
        required: false
        default: ""

jobs:
  run:
    runs-on: ubuntu-latest
    env:
      NFL_COOKIE: ${{ secrets.NFL_COOKIE }}

    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.12"

      - name: Install deps
        run: |
          python -m pip install --upgrade pip
          if [ -f requirements.txt ]; then pip install -r requirements.txt; fi

      # Optional: Cookie aus Secret in Datei spiegeln (falls dein cookieString.py Datei bevorzugt)
      - name: Write cookie file
        run: |
          mkdir -p data
          if [ -n "${NFL_COOKIE}" ]; then
            printf "%s" "${NFL_COOKIE}" > data/nfl_cookie.txt
          fi

      - name: Run scraper
        env:
          MODE: ${{ inputs.mode }}
          LEAGUE_ID: ${{ inputs.league_id }}
          LEAGUE_START_YEAR: ${{ inputs.start_year }}
          LEAGUE_END_YEAR: ${{ inputs.end_year }}
          SLEEPER_LEAGUE_ID: ${{ inputs.sleeper_league_id }}
        run: |
          set -e
          echo "Mode: ${MODE}"

          if [ "${MODE}" = "nfl_drafts" ]; then
            # Cookie-Precheck ohne Heredoc
            python -c "import os,sys; from cookieString import get_session; s=get_session(); url='https://fantasy.nfl.com/league/{}/history/{}/standings'.format(os.environ.get('LEAGUE_ID'), os.environ.get('LEAGUE_END_YEAR')); r=s.get(url, timeout=30, allow_redirects=True); bad=any(w in r.text.lower() for w in ('sign in','signin','login')); print('HTTP:', r.status_code, 'Login?', bad); sys.exit(1 if bad else 0)"
            # NFL.com Drafts scrapen (BeautifulSoup)
            python scrapeNFLDraftboards.py
          else
            # Sleeper Drafts scrapen (API)
            if [ -z "${SLEEPER_LEAGUE_ID}" ]; then
              echo "Bitte sleeper_league_id angeben."; exit 1;
            fi
            python scrapeSleeperDraftboards.py
          fi

      - name: Upload outputs (CSV/TSV + Debug HTML)
        if: always()   # l채dt auch bei vorherigen Fehlern
        uses: actions/upload-artifact@v4
        with:
          name: "draft-output_${{ github.run_number }}"
          path: |
            output/**/*.csv
            output/**/*.tsv
            debug/**/*.html
            debug/**/*.json
          if-no-files-found: ignore
          compression-level: 6
          overwrite: true
