name: NFL Draft Scraping

on:
  workflow_dispatch:
    inputs:
      mode:
        description: "Was scrapen?"
        required: true
        default: "nfl_drafts"
        type: choice
        options:
          - nfl_drafts
          - sleeper_drafts
      start_year:
        description: "Startjahr (nur für NFL.com)"
        required: true
        default: "2015"
      end_year:
        description: "Endjahr (nur für NFL.com)"
        required: true
        default: "2021"
      league_id:
        description: "NFL.com League ID (z.B. 3082897)"
        required: true
        default: "3082897"
      sleeper_league_id:
        description: "Sleeper League ID (falls Sleeper gewählt)"
        required: false
        default: ""

jobs:
  run:
    runs-on: ubuntu-latest
    env:
      NFL_COOKIE: ${{ secrets.NFL_COOKIE }}

    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.12"

      - name: Install deps
        run: |
          python -m pip install --upgrade pip
          if [ -f requirements.txt ]; then pip install -r requirements.txt; else pip install requests beautifulsoup4; fi

      - name: Write cookie file (optional)
        run: |
          mkdir -p data
          if [ -n "${NFL_COOKIE}" ]; then
            printf "%s" "${NFL_COOKIE}" > data/nfl_cookie.txt
          fi

      # >>> Precheck direkt auf die DRAFT-Seite <<<
      - name: Precheck NFL cookie (Draft page)
        if: ${{ inputs.mode == 'nfl_drafts' }}
        run: |
          python - << 'PY'
import os, sys, pathlib, urllib.parse
from cookieString import get_session

lid = os.environ.get("LEAGUE_ID")
yr  = os.environ.get("LEAGUE_END_YEAR")  # nimm zum Test das jüngste Jahr
base = f"https://fantasy.nfl.com/league/{lid}/history/{yr}/draftresults"
qs   = {"draftResultsDetail":"0","draftResultsTab":"round","draftResultsType":"results"}
url  = base + "?" + urllib.parse.urlencode(qs)

s = get_session()
r = s.get(url, timeout=30, allow_redirects=True)
print("HTTP:", r.status_code, "URL:", r.url)

pathlib.Path("debug").mkdir(exist_ok=True, parents=True)
pathlib.Path("debug/draft_precheck.html").write_text(r.text, encoding="utf-8")

txt = r.text.lower()
hard_redirect = ("/login" in r.url) or ("/signin" in r.url)
looks_like_draft = any(w in txt for w in ("draft results","round","overall pick","drafted by","draft grade","pick"))

if hard_redirect or not looks_like_draft:
    sys.exit("Cookie ungültig/abgelaufen → siehe debug/draft_precheck.html")
print("OK: Draftseite erreichbar & authentifiziert.")
PY

      - name: Run scraper
        env:
          MODE: ${{ inputs.mode }}
          LEAGUE_ID: ${{ inputs.league_id }}
          LEAGUE_START_YEAR: ${{ inputs.start_year }}
          LEAGUE_END_YEAR: ${{ inputs.end_year }}
          SLEEPER_LEAGUE_ID: ${{ inputs.sleeper_league_id }}
        run: |
          set -e
          echo "Mode: ${MODE}"
          if [ "${MODE}" = "nfl_drafts" ]; then
            python scrapeNFLDraftboards.py
          else
            if [ -z "${SLEEPER_LEAGUE_ID}" ]; then
              echo "Bitte sleeper_league_id angeben."; exit 1
            fi
            python scrapeSleeperDraftboards.py
          fi

      - name: Upload outputs (CSV/TSV + Debug HTML)
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: "draft-output_${{ github.run_number }}"
          path: |
            output/**/*.csv
            output/**/*.tsv
            debug/**/*.html
            debug/**/*.json
          if-no-files-found: ignore
          compression-level: 6
          overwrite: true
