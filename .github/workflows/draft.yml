name: NFL Draft Scraping

on:
  workflow_dispatch:
    inputs:
      mode:
        description: "Was scrapen?"
        required: true
        default: "nfl_drafts"
        type: choice
        options:
          - nfl_drafts      # NFL.com History-Drafts mit BeautifulSoup
          - sleeper_drafts  # Sleeper Drafts via API
      start_year:
        description: "Startjahr (nur f체r NFL.com)"
        required: true
        default: "2015"
      end_year:
        description: "Endjahr (nur f체r NFL.com)"
        required: true
        default: "2021"
      league_id:
        description: "NFL.com League ID (z.B. 3082897)"
        required: true
        default: "3082897"
      sleeper_league_id:
        description: "Sleeper League ID (falls Sleeper gew채hlt)"
        required: false
        default: ""

jobs:
  run:
    runs-on: ubuntu-latest
    env:
      NFL_COOKIE: ${{ secrets.NFL_COOKIE }}

    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.12"

      - name: Install deps
        run: |
          python -m pip install --upgrade pip
          if [ -f requirements.txt ]; then pip install -r requirements.txt; fi

      # Optional: Cookie aus Secret in Datei spiegeln (falls dein cookieString.py Datei bevorzugt)
      - name: Write cookie file
        run: |
          mkdir -p data
          if [ -n "${NFL_COOKIE}" ]; then
            printf "%s" "${NFL_COOKIE}" > data/nfl_cookie.txt
          fi

      - name: Run scraper
        env:
          MODE: ${{ inputs.mode }}
          LEAGUE_ID: ${{ inputs.league_id }}
          LEAGUE_START_YEAR: ${{ inputs.start_year }}
          LEAGUE_END_YEAR: ${{ inputs.end_year }}
          SLEEPER_LEAGUE_ID: ${{ inputs.sleeper_league_id }}
        run: |
          set -e
          echo "Mode: ${MODE}"
          if [ "${MODE}" = "nfl_drafts" ]; then
            # NFL.com History Drafts (BeautifulSoup)
            python - << 'PY'
          from cookieString import get_session
          import os, sys
          # schneller Cookie-/ENV-Check
          miss = [k for k in ("LEAGUE_ID","LEAGUE_START_YEAR","LEAGUE_END_YEAR") if not os.getenv(k)]
          if miss: sys.exit(f"Fehlende ENV: {', '.join(miss)}")
          s = get_session()
          url = f"https://fantasy.nfl.com/league/{os.environ['LEAGUE_ID']}/history/{os.environ['LEAGUE_END_YEAR']}/standings"
          r = s.get(url, timeout=30, allow_redirects=True)
          bad = any(w in r.text.lower() for w in ("sign in","signin","login"))
          print("HTTP:", r.status_code, "Login-Seite?", bad)
          if bad: sys.exit("NFL_COOKIE scheint ung체ltig/abgelaufen.")
          PY
            python scrapeNFLDraftboards.py
          else
            # Sleeper Drafts
            if [ -z "${SLEEPER_LEAGUE_ID}" ]; then
              echo "Bitte sleeper_league_id angeben."; exit 1;
            fi
            python scrapeSleeperDraftboards.py
          fi

      - name: Upload outputs (output + debug)
        uses: actions/upload-artifact@v4
        with:
          name: draft-output
          path: |
            output/**
            debug/**
          if-no-files-found: warn

            # Sleeper Drafts
            if [ -z "${SLEEPER_LEA_]()
